## Run custom Node application
## Add health probes
## Run rolling upgrade
## Leverage ConfigMap

from directory k8s/sharks-on-node:

kubectl create namespace sharks

kubectl create configmap sharks-config --from-literal=MY_SPECIAL_KEY='a great new day' --from-literal=OTHER_KEY=42  -n=sharks

kubectl apply -f .\deployment.yaml
kubectl apply -f .\service.yaml

kubectl get svc -n=sharks

# check that the sharks application is live and can be accessed from the browser or using curl
# verify that the environment variable $MY_SPECIAL_VAR has been set - with the value specified in the ConfigMap
# this can be done using kubectl exec, from the K8S Dashboard, from WeaveScope


# define health probes
kubectl apply -f .\deployment_v2.yaml

# note: the application does not currently support these probes, so the new container will never be healthy and the upgrade is stopped
kubectl describe deployment sharks-app -n=sharks

# and check in Dashboard
# deployment has two replicacontrollers for the two versions of the Deployment; the original one has three Pods, the new one just one (in an unhealthy state)

# build the new version of the Sharks on Node App image 
# with support for health probes 

docker build -t nodejs-image-demo-v2 .
docker images

docker run --name my-node-app-v2 -p 8302:8080 -d nodejs-image-demo-v2

docker tag nodejs-image-demo-v2 lucasjellema/sharks-app:2.0
docker push lucasjellema/sharks-app:2.0

# deployment_v3 uses the new image that does provide the health probes; likely this upgrade is more successful
kubectl apply -f .\deployment_v3.yaml


kubectl describe deployment sharks-app -n=sharks

# exec into one of the containers
# do a ps -ef
# kill the node application's process
# check the status of the deployment - for example in the K8S Dashboard or with

kubectl describe deployment sharks-app -n=sharks

# since the probes are failing, a new Pod is started and the failing one removed